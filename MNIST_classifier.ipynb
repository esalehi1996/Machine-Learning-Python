{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chunk of the code, imports all the necessary libraries and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will read all the training data from the images. Then we have to perform denoising on them using the median filter. afterwards all image data will be saved in a dataframe which will then be saved in a csv file named \"train_denoised.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['label' ] + list(range(0, 784))\n",
    "df = pd.DataFrame( columns = columns)\n",
    "\n",
    "j = 0\n",
    "for i in tqdm(range(0,10)):\n",
    "    directory = os.fsencode(\"training/\" + str(i) + \"/\")\n",
    "    ls = []\n",
    "    for file in tqdm(os.listdir(directory)):\n",
    "        filename = os.fsdecode(file)\n",
    "        img = mpimg.imread(\"training/\" + str(i) + \"/\" + filename) \n",
    "        img = ndimage.median_filter(img , size=3)\n",
    "        img = img.ravel()\n",
    "        row = [i] \n",
    "        row.extend(img)\n",
    "        ls.append(row)\n",
    "    df_tmp = pd.DataFrame(data = ls  , columns = columns)\n",
    "    frames = [df, df_tmp]\n",
    "    df = pd.concat(frames)\n",
    "df.to_csv('train_denoised.csv', encoding='utf-8' , index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we will only work with the training data in the dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('train_denoised.csv')\n",
    "   \n",
    "y = train['label']\n",
    "X = train[list(train)[1:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import all necessary sklearn dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do dimensionality reduction, we will use PCA and LDA. PCA performs better though and will be used for reporting final results. Now, we will obtain the transformed data using PCA and LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erfan\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "C:\\Users\\Erfan\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X)\n",
    "lda = LinearDiscriminantAnalysis(n_components=50)\n",
    "X_lda = lda.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, we will use validation techniques to estimate test error and select the best model. We will randomly split the data and use 20 percent of data for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size = 0.20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use the obtained training data to fit a knn model. Through expermentation, we have concluded that 7 is suitable for number of neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the obtained model to estimate the test error. We will use the validation set to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1174    3    0    0    0    1    6    1    0    1]\n",
      " [   0 1358    4    0    0    0    0    4    0    1]\n",
      " [  10    2 1160    0    1    2    2   12    3    2]\n",
      " [   1    1    5 1194    0   13    1   10    6    5]\n",
      " [   1    5    1    0 1103    0    5    1    0   18]\n",
      " [   3    7    3   12    2 1016   14    2    4    4]\n",
      " [   6    3    0    0    0    3 1160    0    0    0]\n",
      " [   0    8    5    0    6    1    0 1246    0   13]\n",
      " [   2   11    4   14    4   12    8    3 1096   12]\n",
      " [   2    0    0   15   15    2    0   16    5 1144]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98      1186\n",
      "          1       0.97      0.99      0.98      1367\n",
      "          2       0.98      0.97      0.98      1194\n",
      "          3       0.97      0.97      0.97      1236\n",
      "          4       0.98      0.97      0.97      1134\n",
      "          5       0.97      0.95      0.96      1067\n",
      "          6       0.97      0.99      0.98      1172\n",
      "          7       0.96      0.97      0.97      1279\n",
      "          8       0.98      0.94      0.96      1166\n",
      "          9       0.95      0.95      0.95      1199\n",
      "\n",
      "avg / total       0.97      0.97      0.97     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained a precision of 97 percent which is really good. Next, We will plot the ROC curve for this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train a random forest with 100 estimators. The criterion for decision making will be \"entropy\" instead of \"gini\" and the classifier will be trained using the maximum number of CPU threads to maximize training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(criterion = \"entropy\" , n_jobs=-1, n_estimators=50)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will validate the model using validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1149    1    2    4    0    5   14    2    7    2]\n",
      " [   0 1344    6    3    1    0    3    4    3    3]\n",
      " [   6    1 1130   17    7    2    4   12   14    1]\n",
      " [   3    1   22 1142    0   17    3   14   22   12]\n",
      " [   1    7    5    0 1076    1   14    2    4   24]\n",
      " [   6    1    5   19    7 1003   17    4    3    2]\n",
      " [  14    1   10    0    3    4 1139    1    0    0]\n",
      " [   0    7   14    1    9    3    0 1221    4   20]\n",
      " [   2   11   13   29    6   12   14    7 1059   13]\n",
      " [   0    2    7   33   30   10    1   31    6 1079]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      1186\n",
      "          1       0.98      0.98      0.98      1367\n",
      "          2       0.93      0.95      0.94      1194\n",
      "          3       0.92      0.92      0.92      1236\n",
      "          4       0.94      0.95      0.95      1134\n",
      "          5       0.95      0.94      0.94      1067\n",
      "          6       0.94      0.97      0.96      1172\n",
      "          7       0.94      0.95      0.95      1279\n",
      "          8       0.94      0.91      0.93      1166\n",
      "          9       0.93      0.90      0.92      1199\n",
      "\n",
      "avg / total       0.95      0.95      0.95     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfc.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtainded a precision of 95 percent which is very good. Next we will plot the ROC curve for this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train a Support Vector Classifier using the Gaussian kernel and the mapped training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf' )\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the trained model and the validation data, to estimate the test error of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1173    1    1    1    1    4    1    0    2    2]\n",
      " [   0 1352    6    1    1    0    0    5    1    1]\n",
      " [   5    1 1170    3    4    1    1    6    3    0]\n",
      " [   1    0    7 1199    0   12    0    9    7    1]\n",
      " [   3    5    2    0 1096    1    6    2    1   18]\n",
      " [   1    1    4   13    4 1030    8    0    3    3]\n",
      " [   8    0    2    0    0    6 1156    0    0    0]\n",
      " [   0    4    6    0    8    1    0 1253    0    7]\n",
      " [   1    5    4    8    2    6    5    2 1129    4]\n",
      " [   2    2    0    7   20    3    0   17    5 1143]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      1186\n",
      "          1       0.99      0.99      0.99      1367\n",
      "          2       0.97      0.98      0.98      1194\n",
      "          3       0.97      0.97      0.97      1236\n",
      "          4       0.96      0.97      0.97      1134\n",
      "          5       0.97      0.97      0.97      1067\n",
      "          6       0.98      0.99      0.98      1172\n",
      "          7       0.97      0.98      0.97      1279\n",
      "          8       0.98      0.97      0.97      1166\n",
      "          9       0.97      0.95      0.96      1199\n",
      "\n",
      "avg / total       0.98      0.98      0.98     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained a precision of 98 percent which is best among the three models.  Afterwards, we will plot the ROC curve for this classifier aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the pickle library to save the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(knn, open(\"KNN_model\", 'wb'))\n",
    "pickle.dump(rfc, open(\"Random_forest_model\", 'wb'))\n",
    "pickle.dump(svclassifier, open(\"SVC_RBF_model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
